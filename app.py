import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import os
import time

from energy_env import SmartBuildingEnv
from ppo_agent import PPOAgent
from training import train_ppo
from utils import load_data, preprocess_data, calculate_metrics
from visualization import (
    plot_energy_consumption,
    plot_comfort_metrics,
    plot_optimization_comparison,
    plot_training_progress,
    plot_control_actions
)
from building_3d_vis import (
    create_building_model,
    plot_building_3d,
    plot_building_animation,
    update_building_state
)

# Set page configuration
st.set_page_config(
    page_title="Optimisation √ânerg√©tique des B√¢timents Intelligents",
    page_icon="üè¢",
    layout="wide"
)

# Main title
st.title("üè¢ Syst√®me d'Optimisation √ânerg√©tique pour B√¢timents Intelligents")
st.markdown("### Utilisation de l'Apprentissage par Renforcement PPO pour √âquilibrer √ânergie et Confort")

# Sidebar
st.sidebar.title("Configuration")

# Create two main tabs in the sidebar
sidebar_tab1, sidebar_tab2 = st.sidebar.tabs(["Optimisation", "Documentation"])

with sidebar_tab1:
    # Data loading section
    st.header("1. Source de Donn√©es")
    data_source = st.radio(
        "S√©lectionnez une source de donn√©es:",
        ["Donn√©es √âchantillon", "Donn√©es B√¢timent Personnalis√©es", "T√©l√©charger Donn√©es"]
    )

# Function to display the documentation content
def show_documentation():
    # Application information
    st.header("√Ä propos de cette Application")
    st.write("""
    Cette application utilise l'Optimisation par Politique Proximale (PPO), un algorithme d'apprentissage par renforcement de pointe, 
    pour optimiser la consommation d'√©nergie dans les b√¢timents intelligents tout en maintenant les niveaux de confort des occupants.
    
    ### Fonctionnalit√©s Cl√©s:
    - **Analyse de Donn√©es:** Analyse des mod√®les de consommation d'√©nergie √† partir des donn√©es du b√¢timent
    - **Apprentissage par Renforcement:** Mise en ≈ìuvre de PPO pour optimiser les strat√©gies de contr√¥le
    - **√âquilibre de Confort:** √âquilibre entre les √©conomies d'√©nergie et le confort des occupants
    - **Visualisation:** Visualisation des mod√®les de consommation et des r√©sultats d'optimisation
    
    ### Comment cela fonctionne:
    1. T√©l√©chargez les donn√©es de consommation d'√©nergie de votre b√¢timent intelligent
    2. Configurez les param√®tres de confort et les param√®tres d'entra√Ænement
    3. Entra√Ænez l'agent PPO pour apprendre des strat√©gies de contr√¥le optimales
    4. Examinez les r√©sultats montrant les √©conomies d'√©nergie et les mesures de confort
    """)
    
    # Theoretical explanation section
    st.header("Fondements Th√©oriques de l'Apprentissage par Renforcement")
    
    # RL Basics Tab
    tab1, tab2, tab3, tab4 = st.tabs(["Concepts de Base", "Mod√©lisation de l'Environnement", "Algorithme PPO", "Hyperparam√®tres"])
    
    with tab1:
        st.subheader("Concepts Fondamentaux de l'Apprentissage par Renforcement")
        st.markdown("""
        L'apprentissage par renforcement (RL) est un paradigme d'apprentissage automatique o√π un agent apprend √† prendre des d√©cisions 
        en interagissant avec un environnement pour maximiser une r√©compense cumulative.
        
        #### Composants Cl√©s
        
        1. **Agent**: L'entit√© qui prend des d√©cisions (dans notre cas, le contr√¥leur du b√¢timent).
        
        2. **Environnement**: Le syst√®me avec lequel l'agent interagit (le b√¢timent intelligent).
        
        3. **√âtat (S)**: Une repr√©sentation de la situation actuelle de l'environnement.
           - Dans notre application: `[outside_temp, indoor_temp, indoor_humidity, light_level, occupancy, hour_sin, hour_cos, day_sin, day_cos]`
        
        4. **Action (A)**: Les d√©cisions que l'agent peut prendre.
           - Dans notre application: `[hvac_adjustment, lighting_adjustment]`
        
        5. **R√©compense (R)**: Le signal de feedback que l'agent re√ßoit apr√®s chaque action.
           - Dans notre application: `reward = (1 - comfort_weight) * energy_reward + comfort_weight * comfort_score`
        
        6. **Politique (œÄ)**: La strat√©gie que l'agent utilise pour d√©terminer ses actions.
           - Dans notre application: Une politique stochastique repr√©sent√©e par un r√©seau de neurones.
        
        #### Processus de D√©cision de Markov (MDP)
        
        Le probl√®me d'optimisation √©nerg√©tique est formul√© comme un MDP, o√π:
        - Les transitions d'√©tat suivent la propri√©t√© de Markov (l'√©tat futur d√©pend uniquement de l'√©tat actuel et de l'action).
        - L'objectif est de trouver une politique optimale qui maximise la r√©compense cumul√©e attendue.
        
        #### D√©fis Sp√©cifiques √† l'Optimisation √ânerg√©tique
        
        - **Compromis Multi-objectifs**: √âquilibrer l'√©conomie d'√©nergie et le confort des occupants.
        - **Dynamique Temporelle**: Les conditions ext√©rieures et l'occupation varient dans le temps.
        - **Contraintes Op√©rationnelles**: Respecter les limites physiques des syst√®mes CVC et d'√©clairage.
        """)
    
    with tab2:
        st.subheader("Mod√©lisation de l'Environnement et des R√©compenses")
        st.markdown("""
        #### Structure de l'Environnement (SmartBuildingEnv)
        
        Notre environnement `SmartBuildingEnv` h√©rite de l'interface gym.Env et impl√©mente:
        
        1. **Espace d'√âtats**: 
           - Variables continues repr√©sentant les conditions du b√¢timent et le temps.
           - Normalisation des caract√©ristiques pour am√©liorer l'apprentissage.
        
        2. **Espace d'Actions**: 
           - Actions continues pour les ajustements HVAC et d'√©clairage.
           - Plage d'action [-1, 1] mise √† l'√©chelle pour repr√©senter des ajustements de ¬±20%.
        
        3. **Fonction de Transition**: 
           - Simule comment les actions affectent l'environnement int√©rieur.
           - Mod√©lise les changements de temp√©rature et d'√©clairage en fonction des contr√¥les.
        
        #### Conception de la Fonction de R√©compense
        
        La fonction de r√©compense est cruciale pour guider l'apprentissage de l'agent:
        
        ```python
        # Calcul des composantes de la r√©compense
        energy_reward = energy_saved / original_total_power
        comfort_score = (temp_comfort + light_comfort) / 2
        
        # R√©compense pond√©r√©e combinant √©conomie d'√©nergie et confort
        reward = (1 - comfort_weight) * energy_reward + comfort_weight * comfort_score
        ```
        
        #### M√©triques de Confort
        
        1. **Confort Thermique**: 
           - Score maximal (1.0) dans la plage de temp√©rature confortable.
           - D√©croissance lin√©aire en dehors de cette plage.
        
        2. **Confort d'√âclairage**: 
           - Score maximal (1.0) dans la plage de lux confortable.
           - Importance variable selon l'occupation.
        
        Cette conception de r√©compense permet √† l'agent de trouver un √©quilibre optimal entre:
        - Minimiser la consommation d'√©nergie (√©conomiser des ressources)
        - Maintenir des conditions de confort acceptables (satisfaire les occupants)
        """)
    
    with tab3:
        st.subheader("Algorithme PPO (Proximal Policy Optimization)")
        st.markdown("""
        PPO est un algorithme d'apprentissage par renforcement avanc√© qui combine:
        
        #### Architecture Acteur-Critique
        
        Notre impl√©mentation utilise une architecture r√©seau avec deux composants:
        
        1. **Acteur (Politique)**: 
           - D√©termine quelles actions prendre dans un √©tat donn√©.
           - Produit une distribution de probabilit√© sur les actions possibles.
           - Dans notre cas: distribution gaussienne avec moyenne et √©cart-type.
        
        2. **Critique (Fonction de Valeur)**: 
           - √âvalue la "valeur" d'un √©tat.
           - Aide √† r√©duire la variance lors de l'apprentissage.
        
        #### Avantages de PPO
        
        1. **Stabilit√©**: 
           - Utilise un ratio de probabilit√© limit√© pour √©viter les mises √† jour de politique trop grandes.
           - Le param√®tre `clip_param` (0.2 dans notre impl√©mentation) contr√¥le cette limitation.
        
        2. **√âchantillonnage Efficace**: 
           - R√©utilise les exp√©riences pass√©es via l'apprentissage hors politique.
           - Plus efficace en termes d'√©chantillons que les m√©thodes de politique sur politique.
        
        3. **Apprentissage Continu**: 
           - Prend en charge les espaces d'action continus n√©cessaires pour le contr√¥le CVC et d'√©clairage.
        
        #### Fonction Objectif de PPO
        
        ```
        L_CLIP(Œ∏) = √ä_t [ min(r_t(Œ∏) * A_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ) * A_t) ]
        ```
        
        - `r_t(Œ∏)` est le ratio de probabilit√© entre la nouvelle et l'ancienne politique.
        - `A_t` est l'estimation de l'avantage.
        - `Œµ` est le param√®tre de clip (0.2 dans notre cas).
        
        #### Estimation d'Avantage G√©n√©ralis√©e (GAE)
        
        Notre impl√©mentation utilise GAE pour estimer l'avantage d'une action:
        
        ```python
        delta = rewards[t] + gamma * next_value_t * next_non_terminal - values[t]
        gae = delta + gamma * gae_lambda * next_non_terminal * gae
        ```
        
        Avec:
        - `gamma` = 0.99 (facteur d'actualisation)
        - `gae_lambda` = 0.95 (param√®tre de compromis biais-variance)
        """)
    
    with tab4:
        st.subheader("Hyperparam√®tres et leur Influence")
        st.markdown("""
        #### Hyperparam√®tres de PPO
        
        | Param√®tre | Valeur | Influence |
        |-----------|--------|-----------|
        | `learning_rate` | Variable (UI) | Taille des pas d'optimisation; impacte la vitesse et la stabilit√© de l'apprentissage |
        | `batch_size` | Variable (UI) | Nombre d'√©chantillons utilis√©s pour chaque mise √† jour; affecte l'efficacit√© et la stabilit√© |
        | `gamma` | 0.99 | Facteur d'actualisation; d√©termine l'importance des r√©compenses futures vs imm√©diates |
        | `gae_lambda` | 0.95 | Coefficient GAE; contr√¥le le compromis biais-variance dans l'estimation d'avantage |
        | `clip_param` | 0.2 | Limite les changements de politique; aide √† la stabilit√© de l'apprentissage |
        | `entropy_coef` | 0.01 | Encourage l'exploration; emp√™che la convergence pr√©matur√©e |
        | `value_loss_coef` | 0.5 | Poids de la perte de la fonction de valeur vs la perte de politique |
        | `max_grad_norm` | 0.5 | √âcr√™tage du gradient; pr√©vient les explosions de gradient |
        | `ppo_epochs` | 10 | Nombre d'√©poques par lot; plus d'√©poques = plus d'extraction d'information des donn√©es |
        
        #### Param√®tres d'Environnement
        
        | Param√®tre | Valeur | Influence |
        |-----------|--------|-----------|
        | `comfort_weight` | Variable (UI) | √âquilibre entre √©conomie d'√©nergie et confort; valeurs plus √©lev√©es priorisent le confort |
        | `temp_range` | Variable (UI) | Plage de temp√©rature confortable; plage plus large = plus facile √† satisfaire |
        | `light_range` | Variable (UI) | Plage d'√©clairage confortable; plage plus large = plus facile √† satisfaire |
        
        #### Conseils pour l'Optimisation des Hyperparam√®tres
        
        1. **Learning Rate**: 
           - Trop √©lev√©: apprentissage instable
           - Trop bas: apprentissage lent
           - Recommandation: commencer avec 0.0005 et ajuster
        
        2. **Comfort Weight**: 
           - Proche de 0: priorit√© maximale √† l'√©conomie d'√©nergie, potentiellement au d√©triment du confort
           - Proche de 1: priorit√© maximale au confort, potentiellement sans √©conomies significatives
           - Recommandation: 0.5-0.7 pour un bon √©quilibre
        
        3. **Batch Size**: 
           - Plus grand: estimations plus stables mais moins d'updates
           - Plus petit: plus d'updates mais plus de variance
           - Recommandation: 64-128 pour un bon √©quilibre
        
        4. **Nombre d'√âpoques**: 
           - Plus √©lev√©: meilleure extraction d'information mais risque de surapprentissage
           - Recommandation: 30-100 pour des r√©sultats significatifs
        """)
    
    # Add code explanation section
    st.header("Structure du Code et Impl√©mentation")
    
    code_tab1, code_tab2, code_tab3 = st.tabs(["Architecture Globale", "Environnement RL", "Agent PPO"])
    
    with code_tab1:
        st.markdown("""
        ### Organisation du Code
        
        Le projet est structur√© en plusieurs modules avec des responsabilit√©s sp√©cifiques:
        
        1. **app.py**: 
           - Interface utilisateur Streamlit
           - Contr√¥le du flux de l'application
           - Visualisation des r√©sultats
        
        2. **energy_env.py**: 
           - D√©finition de l'environnement `SmartBuildingEnv`
           - Mod√©lisation des dynamiques du b√¢timent
           - Calcul des r√©compenses
        
        3. **ppo_agent.py**: 
           - Impl√©mentation de l'algorithme PPO
           - Architecture r√©seau acteur-critique
           - Logique d'apprentissage et d'inf√©rence
        
        4. **training.py**: 
           - Boucle d'entra√Ænement
           - Collecte des m√©triques
           - Gestion des √©pisodes
        
        5. **utils.py**: 
           - Chargement et pr√©traitement des donn√©es
           - Calcul des m√©triques de performance
        
        6. **visualization.py**: 
           - G√©n√©ration de graphiques pour les donn√©es et r√©sultats
        
        Cette s√©paration modulaire facilite la maintenance et l'extension du code, et permet de tester individuellement chaque composant.
        """)
        
    with code_tab2:
        st.code("""
# Extrait simplifi√© de energy_env.py
class SmartBuildingEnv(gym.Env):
    def __init__(self, config):
        # D√©finir les espaces d'√©tat et d'action
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9,))
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,))
        
        # Stocker la configuration
        self.data = config["data"]
        self.comfort_weight = config["comfort_weight"]
        self.temp_range = config["temp_range"]
        self.light_range = config["light_range"]
    
    def reset(self):
        # R√©initialiser l'environnement
        self.current_idx = 0
        self.state = self._get_observation(self.current_idx)
        return self.state
    
    def step(self, action):
        # Appliquer les actions au b√¢timent
        hvac_adjustment = action[0] * 0.2  # Mise √† l'√©chelle
        lighting_adjustment = action[1] * 0.2
        
        # Calculer les nouvelles conditions du b√¢timent
        adjusted_hvac_power = current_data["hvac_power"] * (1 + hvac_adjustment)
        adjusted_lighting_power = current_data["lighting_power"] * (1 + lighting_adjustment)
        
        # Calculer le confort et les √©conomies d'√©nergie
        energy_saved = original_total_power - adjusted_total_power
        comfort_score = (temp_comfort + light_comfort) / 2
        
        # Calculer la r√©compense
        energy_reward = energy_saved / original_total_power
        reward = (1 - self.comfort_weight) * energy_reward + self.comfort_weight * comfort_score
        
        # Avancer √† l'√©tat suivant
        self.current_idx += 1
        done = self.current_idx >= self.max_idx
        self.state = self._get_observation(self.current_idx) if not done else self.state
        
        return self.state, reward, done, info
        """, language='python')
        
    with code_tab3:
        st.code("""
# Extrait simplifi√© de ppo_agent.py
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(ActorCritic, self).__init__()
        
        # R√©seaux de l'acteur (politique)
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )
        self.actor_mean = nn.Linear(hidden_dim, action_dim)
        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))
        
        # R√©seau du critique (fonction de valeur)
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
    
    def get_action(self, state, deterministic=False):
        # Obtenir la distribution d'action
        action_mean, action_std, _ = self.forward(state)
        
        if deterministic:
            return action_mean
        
        # √âchantillonner une action
        dist = Normal(action_mean, action_std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        
        return torch.tanh(action), log_prob

class PPOAgent:
    def update(self):
        # Calcul des retours et avantages
        returns, advantages = self._compute_gae(rewards, values, dones)
        
        # Boucle d'entra√Ænement PPO
        for _ in range(self.ppo_epochs):
            # Mini-batch training
            for batch_indices in batches:
                # Calcul du ratio de probabilit√©
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                
                # Objectif PPO avec clipping
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # Pertes de la fonction de valeur et de l'entropie
                value_loss = 0.5 * (batch_returns - values_pred).pow(2).mean()
                entropy_loss = -entropy.mean()
                
                # Perte totale
                loss = policy_loss + self.value_loss_coef * value_loss + self.entropy_coef * entropy_loss
                
                # Mise √† jour des param√®tres
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                self.optimizer.step()
        """, language='python')
    
    st.markdown("""
    ## Fin de la Documentation
    
    """)

# Handle documentation tab
with sidebar_tab2:
    st.write("Documentation compl√®te sur l'apprentissage par renforcement.")
    if st.button("Afficher la Documentation"):
        # Create a flag in session state to show documentation
        st.session_state['show_docs'] = True

# Load data
if data_source == "Donn√©es √âchantillon":
    data_path = "data/sample_data.csv"
    df = load_data(data_path)
    st.success("Donn√©es √©chantillon standard charg√©es avec succ√®s!")
elif data_source == "Donn√©es B√¢timent Personnalis√©es":
    data_path = "data/custom_data.csv"
    df = load_data(data_path)
    st.success("Donn√©es personnalis√©es du b√¢timent charg√©es avec succ√®s!")
else:
    uploaded_file = st.file_uploader("T√©l√©chargez un fichier CSV avec des donn√©es de consommation √©nerg√©tique", type="csv")
    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        st.success("Donn√©es t√©l√©charg√©es avec succ√®s!")
    else:
        st.warning("Veuillez t√©l√©charger un fichier de donn√©es pour continuer.")
        df = None

# Initialize show_docs in session state if it doesn't exist
if 'show_docs' not in st.session_state:
    st.session_state['show_docs'] = False

# Main content
if st.session_state.get('show_docs', False):
    # Display documentation when the flag is set
    show_documentation()
    
    # Add button to return to main content
    if st.button("Retour √† l'Application"):
        st.session_state['show_docs'] = False
        st.rerun()
        
elif df is not None:
    # Display data info
    st.header("1. Aper√ßu des Donn√©es")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write("√âchantillon de Donn√©es:")
        st.dataframe(df.head())
    
    with col2:
        st.write("Statistiques des Donn√©es:")
        st.dataframe(df.describe())
    
    # Data preprocessing
    st.header("2. Pr√©traitement des Donn√©es")
    processed_df = preprocess_data(df)
    st.success("Donn√©es pr√©trait√©es avec succ√®s!")
    
    # Display original energy consumption
    st.header("3. Mod√®les Actuels de Consommation d'√ânergie")
    fig = plot_energy_consumption(processed_df)
    st.plotly_chart(fig, use_container_width=True)
    
    # Training configuration
    st.header("4. Configuration de l'Optimisation")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Param√®tres d'Environnement")
        comfort_weight = st.slider("Poids du Confort", 0.0, 1.0, 0.5, 0.05,
                                 help="Poids du confort dans la fonction de r√©compense (plus √©lev√© signifie prioriser le confort)")
        
        temp_range = st.slider("Plage de Temp√©rature Confortable (¬∞C)", 18.0, 28.0, (20.0, 25.0),
                              help="Plage de temp√©rature consid√©r√©e comme confortable pour les occupants")
        
        light_range = st.slider("Plage de Niveau d'√âclairage Confortable (lux)", 300, 800, (400, 700),
                               help="Plage de niveau d'√©clairage consid√©r√©e comme confortable pour les occupants")
    
    with col2:
        st.subheader("Param√®tres d'Apprentissage")
        epochs = st.slider("√âpoques d'Entra√Ænement", 10, 200, 50, 
                          help="Nombre d'√©poques d'entra√Ænement")
        
        learning_rate = st.selectbox(
            "Taux d'Apprentissage", 
            [0.0001, 0.0003, 0.0005, 0.001, 0.002, 0.005],
            index=2,
            help="Taux d'apprentissage pour l'algorithme PPO"
        )
        
        batch_size = st.selectbox(
            "Taille du Batch",
            [16, 32, 64, 128, 256],
            index=2,
            help="Taille du batch pour l'entra√Ænement"
        )
    
    # Training section
    st.header("5. Lancer l'Optimisation")
    
    if st.button("D√©marrer l'Entra√Ænement d'Optimisation", key="train"):
        # Create environment
        env_config = {
            "data": processed_df,
            "comfort_weight": comfort_weight,
            "temp_range": temp_range,
            "light_range": light_range
        }
        
        env = SmartBuildingEnv(env_config)
        
        # Create agent
        agent_config = {
            "state_dim": env.observation_space.shape[0],
            "action_dim": env.action_space.shape[0],
            "lr": learning_rate,
            "batch_size": batch_size
        }
        
        agent = PPOAgent(agent_config)
        
        # Training progress placeholder
        progress_bar = st.progress(0)
        training_metrics_container = st.empty()
        training_plot_container = st.empty()
        
        # Train the agent
        progress = {"epochs": [], "rewards": [], "energy_saved": [], "comfort_score": []}
        
        for i in range(epochs):
            # Training step
            metrics = train_ppo(env, agent, epochs=1)
            
            # Update progress
            progress["epochs"].append(i+1)
            progress["rewards"].append(metrics["avg_reward"])
            progress["energy_saved"].append(metrics["energy_saved"])
            progress["comfort_score"].append(metrics["comfort_score"])
            
            # Update UI
            progress_bar.progress((i+1)/epochs)
            
            # Show current metrics
            training_metrics_container.write(f"√âpoque {i+1}/{epochs} - "
                                            f"R√©compense Moy: {metrics['avg_reward']:.4f}, "
                                            f"√ânergie √âconomis√©e: {metrics['energy_saved']:.2f}%, "
                                            f"Score de Confort: {metrics['comfort_score']:.2f}%")
            
            # Plot training progress
            if (i+1) % 5 == 0 or i == epochs-1:
                fig = plot_training_progress(progress)
                training_plot_container.plotly_chart(fig, use_container_width=True)
        
        st.success(f"Optimisation termin√©e en {epochs} √©poques!")
        
        # Results section
        st.header("6. R√©sultats de l'Optimisation")
        
        # Run optimized policy
        env.reset()
        done = False
        optimized_data = []
        
        with st.spinner("Simulation de la strat√©gie de contr√¥le optimis√©e..."):
            while not done:
                state = env.get_state()
                action = agent.select_action(state)
                next_state, reward, done, info = env.step(action)
                optimized_data.append(info)
        
        # Convert to dataframe
        optimized_df = pd.DataFrame(optimized_data)
        
        # Calculate metrics
        metrics = calculate_metrics(processed_df, optimized_df)
        
        # Display metrics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("√âconomies d'√ânergie", f"{metrics['energy_saved']:.2f}%", "‚Üì Bon")
            
        with col2:
            st.metric("Confort Maintenu", f"{metrics['comfort_score']:.2f}%", "‚Üë Bon")
            
        with col3:
            st.metric("R√©duction des Co√ªts", f"{metrics['cost_saved']:.2f}‚Ç¨", "‚Üì Bon")
        
        # Plot comparison
        st.subheader("Consommation d'√ânergie: Avant vs Apr√®s Optimisation")
        fig = plot_optimization_comparison(processed_df, optimized_df)
        st.plotly_chart(fig, use_container_width=True)
        
        # Plot comfort metrics
        st.subheader("M√©triques de Confort")
        fig = plot_comfort_metrics(processed_df, optimized_df)
        st.plotly_chart(fig, use_container_width=True)
        
        # Plot control actions
        st.subheader("Actions de Contr√¥le Optimis√©es")
        fig = plot_control_actions(optimized_df)
        st.plotly_chart(fig, use_container_width=True)
        
        # 3D Building Visualization
        st.header("Visualisation 3D du B√¢timent Intelligent")
        st.write("""
        Cette visualisation 3D montre comment les actions de l'agent affectent les diff√©rentes pi√®ces du b√¢timent.
        Les couleurs repr√©sentent la temp√©rature (bleu = froid, blanc = neutre, rouge = chaud), 
        la transparence indique le niveau d'√©clairage, et les points verts indiquent la pr√©sence d'occupants.
        """)
        
        # Configuration du b√¢timent
        st.subheader("Configuration du B√¢timent")
        col1, col2, col3 = st.columns(3)
        with col1:
            num_floors = st.slider("Nombre d'√©tages", min_value=1, max_value=5, value=3)
        with col2:
            num_rooms = st.slider("Nombre de pi√®ces par √©tage", min_value=4, max_value=16, value=4, step=4)
        with col3:
            building_type = st.selectbox("Type de b√¢timent", ["bureau", "r√©sidentiel", "commercial", "industriel"])
        
        # Cr√©er le mod√®le du b√¢timent
        building_model = create_building_model(num_floors=num_floors, num_rooms_per_floor=num_rooms, building_type=building_type)
        
        # Afficher le mod√®le initial
        st.subheader("√âtat Initial du B√¢timent")
        st.write("""
        Utilisez les contr√¥les ci-dessous pour explorer le b√¢timent en 3D. Vous pouvez:
        - Faire pivoter le mod√®le en cliquant et faisant glisser
        - Zoomer avec la molette de la souris
        - Voir les d√©tails des pi√®ces en passant le curseur dessus
        - Changer la vue avec les boutons du menu d√©roulant (en haut √† gauche)
        """)
        initial_fig = plot_building_3d(building_model, show_controls=True)
        st.plotly_chart(initial_fig, use_container_width=True)
        
        # L√©gende explicative compl√©mentaire
        legend_col1, legend_col2, legend_col3 = st.columns(3)
        with legend_col1:
            st.markdown("**Couleurs des pi√®ces**")
            st.markdown("- üîµ Bleu: Temp√©rature basse")
            st.markdown("- ‚ö™ Blanc: Temp√©rature neutre")
            st.markdown("- üî¥ Rouge: Temp√©rature √©lev√©e")
        with legend_col2:
            st.markdown("**Autres √©l√©ments**")
            st.markdown("- üü¢ Points verts: Occupants")
            st.markdown("- üîç Transparence: Niveau d'√©clairage")
            st.markdown("- ü™ü Bleu clair: Fen√™tres")
        with legend_col3:
            st.markdown("**Types de pi√®ces**")
            st.markdown("- Bureau: Espaces de travail")
            st.markdown("- Salle de r√©union: R√©unions")
            st.markdown("- Espace commun: Zones partag√©es")
            st.markdown("- Stockage: Rangement, archives")
        
        # Simulation 3D de l'optimisation
        st.subheader("Simulation de l'Optimisation")
        st.write("""
        Cette section montre comment l'agent PPO optimise dynamiquement le b√¢timent au fil du temps.
        Observez comment les temp√©ratures et niveaux d'√©clairage s'ajustent en fonction des actions de l'agent
        et des changements d'occupation.
        """)
        
        # Pour la d√©mo, on va simuler des actions HVAC et √©clairage
        if 'hvac_action' not in optimized_df.columns:
            optimized_df['hvac_action'] = optimized_df.apply(lambda row: np.random.uniform(-0.2, 0.2), axis=1)
        if 'lighting_action' not in optimized_df.columns:
            optimized_df['lighting_action'] = optimized_df.apply(lambda row: np.random.uniform(-0.2, 0.2), axis=1)
        if 'occupancy' not in optimized_df.columns:
            # Cr√©er une colonne d'occupation temporaire pour la visualisation
            optimized_df['occupancy'] = [np.random.randint(0, 2, len(building_model['rooms'])) for _ in range(len(optimized_df))]
        
        # B√¢timent intelligent avanc√© - Simulation et optimisation 3D
        st.subheader("B√¢timent 3D Intelligent - Simulation d'Optimisation")
        
        # Version am√©lior√©e pour r√©pondre aux besoins du projet
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # Visualisation 3D au centre avec contr√¥les avanc√©s
            fig = plot_building_3d(building_model, show_controls=True)
            
            # Ajout d'un titre explicatif sur le graphique
            fig.update_layout(
                title="Simulation du b√¢timent avec l'agent PPO",
                scene=dict(
                    xaxis_title="X (m)",
                    yaxis_title="Y (m)",
                    zaxis_title="Z (m)",
                    aspectmode='cube'
                )
            )
            
            # Afficher la visualisation 3D avanc√©e
            st.plotly_chart(fig, use_container_width=True, key="main_building_view")
            
            # Explications cl√©s sous la visualisation
            st.info("""
            **L√©gende des couleurs:**
            - üîµ **Bleu**: Zones refroidies par l'agent PPO pour √©conomiser l'√©nergie
            - ‚ö™ **Blanc**: Zones √† temp√©rature optimale (√©conomie + confort)
            - üî¥ **Rouge**: Zones qui n√©cessitent du chauffage selon l'occupation
            
            La **transparence** repr√©sente l'intensit√© de l'√©clairage, et les **points verts** indiquent la pr√©sence d'occupants.
            """)
        
        with col2:
            # Statistiques et informations sur les d√©cisions de l'agent PPO
            st.markdown("#### Statistiques du b√¢timent")
            
            # Temp√©rature et √©clairage moyens
            temp_avg = np.mean([room['temperature'] for room in building_model['rooms']])
            light_avg = np.mean([room['light_level'] for room in building_model['rooms']])
            
            # Comptage des pi√®ces occup√©es
            occupied_count = sum([1 for room in building_model['rooms'] if room.get('occupied', 0) == 1])
            
            # Afficher les m√©triques
            st.metric("Temp√©rature moyenne", f"{temp_avg:.1f} ¬∞C")
            st.metric("√âclairage moyen", f"{light_avg:.0f} lux")
            st.metric("Taux d'occupation", f"{occupied_count}/{len(building_model['rooms'])} pi√®ces")
            
            # D√©cisions de l'agent
            st.markdown("#### D√©cisions de l'agent PPO")
            
            # Prendre la derni√®re action de l'agent s'il existe des donn√©es
            if 'hvac_action' in optimized_df.columns and len(optimized_df) > 0:
                last_action_hvac = optimized_df.iloc[-1]['hvac_action']
                action_text = "‚¨áÔ∏è Refroidissement" if last_action_hvac < -0.05 else "‚¨ÜÔ∏è Chauffage" if last_action_hvac > 0.05 else "‚ü∑ Maintien"
                st.metric("Action HVAC", action_text, f"{last_action_hvac:.2f}")
            else:
                st.metric("Action HVAC", "Non disponible")
                
            if 'lighting_action' in optimized_df.columns and len(optimized_df) > 0:
                last_action_light = optimized_df.iloc[-1]['lighting_action']
                light_text = "‚¨áÔ∏è R√©duction" if last_action_light < -0.05 else "‚¨ÜÔ∏è Augmentation" if last_action_light > 0.05 else "‚ü∑ Maintien" 
                st.metric("Action √âclairage", light_text, f"{last_action_light:.2f}")
            else:
                st.metric("Action √âclairage", "Non disponible")
            
            # Afficher les √©conomies d'√©nergie estim√©es
            st.markdown("#### √âconomies r√©alis√©es")
            st.metric("√ânergie √©conomis√©e", f"{metrics['energy_saved']:.1f}%")
            st.metric("Co√ªt √©conomis√©", f"{metrics['cost_saved']:.2f}‚Ç¨")
            st.metric("Confort maintenu", f"{metrics['comfort_score']:.1f}%")
            
        # Explications sur les diff√©rents types de pi√®ces
        with st.expander("Types de pi√®ces et leur comportement"):
            st.markdown("""
            ## Types de pi√®ces dans le b√¢timent
            
            Chaque type de pi√®ce a des besoins √©nerg√©tiques diff√©rents :
            
            - **Bureaux** : N√©cessitent une temp√©rature stable et un bon √©clairage pendant les heures de travail
            - **Salles de r√©union** : Occupation intermittente, n√©cessitent un conditionnement rapide
            - **Espaces communs** : Utilisation variable, souvent plus occup√©s pendant les pauses
            - **Stockage** : Besoins minimaux en chauffage et √©clairage
            
            L'agent PPO apprend √† optimiser chaque espace selon son utilisation sp√©cifique.
            """)
        
        # Conclusions
        st.header("7. Conclusions")
        st.write(f"""
        L'algorithme d'apprentissage par renforcement PPO a optimis√© avec succ√®s la consommation d'√©nergie du b√¢timent 
        avec une r√©duction de **{metrics['energy_saved']:.2f}%** tout en maintenant 
        **{metrics['comfort_score']:.2f}%** du niveau de confort des occupants.
        
        Cela se traduit par environ **{metrics['cost_saved']:.2f}‚Ç¨** d'√©conomies de co√ªts sur la p√©riode de donn√©es.
        
        L'algorithme a appris √†:
        - Ajuster les points de consigne de temp√©rature en fonction des mod√®les d'occupation
        - Optimiser les niveaux d'√©clairage tout au long de la journ√©e
        - R√©duire l'utilisation du CVC pendant les p√©riodes d'inoccupation tout en maintenant la pr√©paration du syst√®me
        """)
else:
    # Display placeholder when no data is loaded
    st.info("Veuillez s√©lectionner ou t√©l√©charger des donn√©es pour d√©marrer le processus d'optimisation.")
    
    # Show application information
    st.header("√Ä propos de cette Application")
    st.write("""
    Cette application utilise l'Optimisation par Politique Proximale (PPO), un algorithme d'apprentissage par renforcement de pointe, 
    pour optimiser la consommation d'√©nergie dans les b√¢timents intelligents tout en maintenant les niveaux de confort des occupants.
    
    ### Fonctionnalit√©s Cl√©s:
    - **Analyse de Donn√©es:** Analyse des mod√®les de consommation d'√©nergie √† partir des donn√©es du b√¢timent
    - **Apprentissage par Renforcement:** Mise en ≈ìuvre de PPO pour optimiser les strat√©gies de contr√¥le
    - **√âquilibre de Confort:** √âquilibre entre les √©conomies d'√©nergie et le confort des occupants
    - **Visualisation:** Visualisation des mod√®les de consommation et des r√©sultats d'optimisation
    
    ### Comment cela fonctionne:
    1. T√©l√©chargez les donn√©es de consommation d'√©nergie de votre b√¢timent intelligent
    2. Configurez les param√®tres de confort et les param√®tres d'entra√Ænement
    3. Entra√Ænez l'agent PPO pour apprendre des strat√©gies de contr√¥le optimales
    4. Examinez les r√©sultats montrant les √©conomies d'√©nergie et les mesures de confort
    
    Commencez en s√©lectionnant une source de donn√©es dans la barre lat√©rale!
    """)
    
    # Add theoretical explanation section
    st.header("Fondements Th√©oriques de l'Apprentissage par Renforcement")
    
    # RL Basics Tab
    tab1, tab2, tab3, tab4 = st.tabs(["Concepts de Base", "Mod√©lisation de l'Environnement", "Algorithme PPO", "Hyperparam√®tres"])
    
    with tab1:
        st.subheader("Concepts Fondamentaux de l'Apprentissage par Renforcement")
        st.markdown("""
        L'apprentissage par renforcement (RL) est un paradigme d'apprentissage automatique o√π un agent apprend √† prendre des d√©cisions 
        en interagissant avec un environnement pour maximiser une r√©compense cumulative.
        
        #### Composants Cl√©s
        
        1. **Agent**: L'entit√© qui prend des d√©cisions (dans notre cas, le contr√¥leur du b√¢timent).
        
        2. **Environnement**: Le syst√®me avec lequel l'agent interagit (le b√¢timent intelligent).
        
        3. **√âtat (S)**: Une repr√©sentation de la situation actuelle de l'environnement.
           - Dans notre application: `[outside_temp, indoor_temp, indoor_humidity, light_level, occupancy, hour_sin, hour_cos, day_sin, day_cos]`
        
        4. **Action (A)**: Les d√©cisions que l'agent peut prendre.
           - Dans notre application: `[hvac_adjustment, lighting_adjustment]`
        
        5. **R√©compense (R)**: Le signal de feedback que l'agent re√ßoit apr√®s chaque action.
           - Dans notre application: `reward = (1 - comfort_weight) * energy_reward + comfort_weight * comfort_score`
        
        6. **Politique (œÄ)**: La strat√©gie que l'agent utilise pour d√©terminer ses actions.
           - Dans notre application: Une politique stochastique repr√©sent√©e par un r√©seau de neurones.
        
        #### Processus de D√©cision de Markov (MDP)
        
        Le probl√®me d'optimisation √©nerg√©tique est formul√© comme un MDP, o√π:
        - Les transitions d'√©tat suivent la propri√©t√© de Markov (l'√©tat futur d√©pend uniquement de l'√©tat actuel et de l'action).
        - L'objectif est de trouver une politique optimale qui maximise la r√©compense cumul√©e attendue.
        
        #### D√©fis Sp√©cifiques √† l'Optimisation √ânerg√©tique
        
        - **Compromis Multi-objectifs**: √âquilibrer l'√©conomie d'√©nergie et le confort des occupants.
        - **Dynamique Temporelle**: Les conditions ext√©rieures et l'occupation varient dans le temps.
        - **Contraintes Op√©rationnelles**: Respecter les limites physiques des syst√®mes CVC et d'√©clairage.
        """)
    
    with tab2:
        st.subheader("Mod√©lisation de l'Environnement et des R√©compenses")
        st.markdown("""
        #### Structure de l'Environnement (SmartBuildingEnv)
        
        Notre environnement `SmartBuildingEnv` h√©rite de l'interface gym.Env et impl√©mente:
        
        1. **Espace d'√âtats**: 
           - Variables continues repr√©sentant les conditions du b√¢timent et le temps.
           - Normalisation des caract√©ristiques pour am√©liorer l'apprentissage.
        
        2. **Espace d'Actions**: 
           - Actions continues pour les ajustements HVAC et d'√©clairage.
           - Plage d'action [-1, 1] mise √† l'√©chelle pour repr√©senter des ajustements de ¬±20%.
        
        3. **Fonction de Transition**: 
           - Simule comment les actions affectent l'environnement int√©rieur.
           - Mod√©lise les changements de temp√©rature et d'√©clairage en fonction des contr√¥les.
        
        #### Conception de la Fonction de R√©compense
        
        La fonction de r√©compense est cruciale pour guider l'apprentissage de l'agent:
        
        ```python
        # Calcul des composantes de la r√©compense
        energy_reward = energy_saved / original_total_power
        comfort_score = (temp_comfort + light_comfort) / 2
        
        # R√©compense pond√©r√©e combinant √©conomie d'√©nergie et confort
        reward = (1 - comfort_weight) * energy_reward + comfort_weight * comfort_score
        ```
        
        #### M√©triques de Confort
        
        1. **Confort Thermique**: 
           - Score maximal (1.0) dans la plage de temp√©rature confortable.
           - D√©croissance lin√©aire en dehors de cette plage.
        
        2. **Confort d'√âclairage**: 
           - Score maximal (1.0) dans la plage de lux confortable.
           - Importance variable selon l'occupation.
        
        Cette conception de r√©compense permet √† l'agent de trouver un √©quilibre optimal entre:
        - Minimiser la consommation d'√©nergie (√©conomiser des ressources)
        - Maintenir des conditions de confort acceptables (satisfaire les occupants)
        """)
    
    with tab3:
        st.subheader("Algorithme PPO (Proximal Policy Optimization)")
        st.markdown("""
        PPO est un algorithme d'apprentissage par renforcement avanc√© qui combine:
        
        #### Architecture Acteur-Critique
        
        Notre impl√©mentation utilise une architecture r√©seau avec deux composants:
        
        1. **Acteur (Politique)**: 
           - D√©termine quelles actions prendre dans un √©tat donn√©.
           - Produit une distribution de probabilit√© sur les actions possibles.
           - Dans notre cas: distribution gaussienne avec moyenne et √©cart-type.
        
        2. **Critique (Fonction de Valeur)**: 
           - √âvalue la "valeur" d'un √©tat.
           - Aide √† r√©duire la variance lors de l'apprentissage.
        
        #### Avantages de PPO
        
        1. **Stabilit√©**: 
           - Utilise un ratio de probabilit√© limit√© pour √©viter les mises √† jour de politique trop grandes.
           - Le param√®tre `clip_param` (0.2 dans notre impl√©mentation) contr√¥le cette limitation.
        
        2. **√âchantillonnage Efficace**: 
           - R√©utilise les exp√©riences pass√©es via l'apprentissage hors politique.
           - Plus efficace en termes d'√©chantillons que les m√©thodes de politique sur politique.
        
        3. **Apprentissage Continu**: 
           - Prend en charge les espaces d'action continus n√©cessaires pour le contr√¥le CVC et d'√©clairage.
        
        #### Fonction Objectif de PPO
        
        ```
        L_CLIP(Œ∏) = √ä_t [ min(r_t(Œ∏) * A_t, clip(r_t(Œ∏), 1-Œµ, 1+Œµ) * A_t) ]
        ```
        
        - `r_t(Œ∏)` est le ratio de probabilit√© entre la nouvelle et l'ancienne politique.
        - `A_t` est l'estimation de l'avantage.
        - `Œµ` est le param√®tre de clip (0.2 dans notre cas).
        
        #### Estimation d'Avantage G√©n√©ralis√©e (GAE)
        
        Notre impl√©mentation utilise GAE pour estimer l'avantage d'une action:
        
        ```python
        delta = rewards[t] + gamma * next_value_t * next_non_terminal - values[t]
        gae = delta + gamma * gae_lambda * next_non_terminal * gae
        ```
        
        Avec:
        - `gamma` = 0.99 (facteur d'actualisation)
        - `gae_lambda` = 0.95 (param√®tre de compromis biais-variance)
        """)
    
    with tab4:
        st.subheader("Hyperparam√®tres et leur Influence")
        st.markdown("""
        #### Hyperparam√®tres de PPO
        
        | Param√®tre | Valeur | Influence |
        |-----------|--------|-----------|
        | `learning_rate` | Variable (UI) | Taille des pas d'optimisation; impacte la vitesse et la stabilit√© de l'apprentissage |
        | `batch_size` | Variable (UI) | Nombre d'√©chantillons utilis√©s pour chaque mise √† jour; affecte l'efficacit√© et la stabilit√© |
        | `gamma` | 0.99 | Facteur d'actualisation; d√©termine l'importance des r√©compenses futures vs imm√©diates |
        | `gae_lambda` | 0.95 | Coefficient GAE; contr√¥le le compromis biais-variance dans l'estimation d'avantage |
        | `clip_param` | 0.2 | Limite les changements de politique; aide √† la stabilit√© de l'apprentissage |
        | `entropy_coef` | 0.01 | Encourage l'exploration; emp√™che la convergence pr√©matur√©e |
        | `value_loss_coef` | 0.5 | Poids de la perte de la fonction de valeur vs la perte de politique |
        | `max_grad_norm` | 0.5 | √âcr√™tage du gradient; pr√©vient les explosions de gradient |
        | `ppo_epochs` | 10 | Nombre d'√©poques par lot; plus d'√©poques = plus d'extraction d'information des donn√©es |
        
        #### Param√®tres d'Environnement
        
        | Param√®tre | Valeur | Influence |
        |-----------|--------|-----------|
        | `comfort_weight` | Variable (UI) | √âquilibre entre √©conomie d'√©nergie et confort; valeurs plus √©lev√©es priorisent le confort |
        | `temp_range` | Variable (UI) | Plage de temp√©rature confortable; plage plus large = plus facile √† satisfaire |
        | `light_range` | Variable (UI) | Plage d'√©clairage confortable; plage plus large = plus facile √† satisfaire |
        
        #### Conseils pour l'Optimisation des Hyperparam√®tres
        
        1. **Learning Rate**: 
           - Trop √©lev√©: apprentissage instable
           - Trop bas: apprentissage lent
           - Recommandation: commencer avec 0.0005 et ajuster
        
        2. **Comfort Weight**: 
           - Proche de 0: priorit√© maximale √† l'√©conomie d'√©nergie, potentiellement au d√©triment du confort
           - Proche de 1: priorit√© maximale au confort, potentiellement sans √©conomies significatives
           - Recommandation: 0.5-0.7 pour un bon √©quilibre
        
        3. **Batch Size**: 
           - Plus grand: estimations plus stables mais moins d'updates
           - Plus petit: plus d'updates mais plus de variance
           - Recommandation: 64-128 pour un bon √©quilibre
        
        4. **Nombre d'√âpoques**: 
           - Plus √©lev√©: meilleure extraction d'information mais risque de surapprentissage
           - Recommandation: 30-100 pour des r√©sultats significatifs
        """)
    
    # Add code explanation section
    st.header("Structure du Code et Impl√©mentation")
    
    code_tab1, code_tab2, code_tab3 = st.tabs(["Architecture Globale", "Environnement RL", "Agent PPO"])
    
    with code_tab1:
        st.markdown("""
        ### Organisation du Code
        
        Le projet est structur√© en plusieurs modules avec des responsabilit√©s sp√©cifiques:
        
        1. **app.py**: 
           - Interface utilisateur Streamlit
           - Contr√¥le du flux de l'application
           - Visualisation des r√©sultats
        
        2. **energy_env.py**: 
           - D√©finition de l'environnement `SmartBuildingEnv`
           - Mod√©lisation des dynamiques du b√¢timent
           - Calcul des r√©compenses
        
        3. **ppo_agent.py**: 
           - Impl√©mentation de l'algorithme PPO
           - Architecture r√©seau acteur-critique
           - Logique d'apprentissage et d'inf√©rence
        
        4. **training.py**: 
           - Boucle d'entra√Ænement
           - Collecte des m√©triques
           - Gestion des √©pisodes
        
        5. **utils.py**: 
           - Chargement et pr√©traitement des donn√©es
           - Calcul des m√©triques de performance
        
        6. **visualization.py**: 
           - G√©n√©ration de graphiques pour les donn√©es et r√©sultats
        
        Cette s√©paration modulaire facilite la maintenance et l'extension du code, et permet de tester individuellement chaque composant.
        """)
        
    with code_tab2:
        st.code("""
# Extrait simplifi√© de energy_env.py
class SmartBuildingEnv(gym.Env):
    def __init__(self, config):
        # D√©finir les espaces d'√©tat et d'action
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(9,))
        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(2,))
        
        # Stocker la configuration
        self.data = config["data"]
        self.comfort_weight = config["comfort_weight"]
        self.temp_range = config["temp_range"]
        self.light_range = config["light_range"]
    
    def reset(self):
        # R√©initialiser l'environnement
        self.current_idx = 0
        self.state = self._get_observation(self.current_idx)
        return self.state
    
    def step(self, action):
        # Appliquer les actions au b√¢timent
        hvac_adjustment = action[0] * 0.2  # Mise √† l'√©chelle
        lighting_adjustment = action[1] * 0.2
        
        # Calculer les nouvelles conditions du b√¢timent
        adjusted_hvac_power = current_data["hvac_power"] * (1 + hvac_adjustment)
        adjusted_lighting_power = current_data["lighting_power"] * (1 + lighting_adjustment)
        
        # Calculer le confort et les √©conomies d'√©nergie
        energy_saved = original_total_power - adjusted_total_power
        comfort_score = (temp_comfort + light_comfort) / 2
        
        # Calculer la r√©compense
        energy_reward = energy_saved / original_total_power
        reward = (1 - self.comfort_weight) * energy_reward + self.comfort_weight * comfort_score
        
        # Avancer √† l'√©tat suivant
        self.current_idx += 1
        done = self.current_idx >= self.max_idx
        self.state = self._get_observation(self.current_idx) if not done else self.state
        
        return self.state, reward, done, info
        """, language='python')
        
    with code_tab3:
        st.code("""
# Extrait simplifi√© de ppo_agent.py
class ActorCritic(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=64):
        super(ActorCritic, self).__init__()
        
        # R√©seaux de l'acteur (politique)
        self.actor = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh()
        )
        self.actor_mean = nn.Linear(hidden_dim, action_dim)
        self.actor_log_std = nn.Parameter(torch.zeros(action_dim))
        
        # R√©seau du critique (fonction de valeur)
        self.critic = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
    
    def get_action(self, state, deterministic=False):
        # Obtenir la distribution d'action
        action_mean, action_std, _ = self.forward(state)
        
        if deterministic:
            return action_mean
        
        # √âchantillonner une action
        dist = Normal(action_mean, action_std)
        action = dist.sample()
        log_prob = dist.log_prob(action).sum(dim=-1, keepdim=True)
        
        return torch.tanh(action), log_prob

class PPOAgent:
    def update(self):
        # Calcul des retours et avantages
        returns, advantages = self._compute_gae(rewards, values, dones)
        
        # Boucle d'entra√Ænement PPO
        for _ in range(self.ppo_epochs):
            # Mini-batch training
            for batch_indices in batches:
                # Calcul du ratio de probabilit√©
                ratio = torch.exp(new_log_probs - batch_old_log_probs)
                
                # Objectif PPO avec clipping
                surr1 = ratio * batch_advantages
                surr2 = torch.clamp(ratio, 1.0 - self.clip_param, 1.0 + self.clip_param) * batch_advantages
                policy_loss = -torch.min(surr1, surr2).mean()
                
                # Pertes de la fonction de valeur et de l'entropie
                value_loss = 0.5 * (batch_returns - values_pred).pow(2).mean()
                entropy_loss = -entropy.mean()
                
                # Perte totale
                loss = policy_loss + self.value_loss_coef * value_loss + self.entropy_coef * entropy_loss
                
                # Mise √† jour des param√®tres
                self.optimizer.zero_grad()
                loss.backward()
                nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
                self.optimizer.step()
        """, language='python')
    
    st.markdown("""
    ## Fin de la Documentation
    
    """)
